---
title: "神经网络模块（nn.Module）"
weight: 3
bookhidden: true
---

# 神经网络模块（nn.Module）

`nn.Module` 是 PyTorch 中所有神经网络模块的基类。理解如何使用 `nn.Module` 是构建深度学习模型的关键。

## 神经网络基础概念

神经网络是一种模仿人脑处理信息方式的计算模型，它由许多相互连接的节点（神经元）组成，这些节点按层次排列。

神经网络的强大之处在于其能够自动从大量数据中学习复杂的模式和特征，无需人工设计特征提取器。

### 神经元（Neuron）

神经元是神经网络的基本单元，它接收输入信号，通过加权求和后与偏置（bias）相加，然后通过激活函数处理以产生输出。

神经元的权重和偏置是网络学习过程中需要调整的参数。

**数学表示**：

$$output = f(\sum_{i=1}^{n} w_i \cdot x_i + b)$$

其中：
- $x_i$ 是输入
- $w_i$ 是权重
- $b$ 是偏置
- $f$ 是激活函数

### 层（Layer）

神经网络由多个层组成，包括：

- **输入层（Input Layer）**：接收原始输入数据
- **隐藏层（Hidden Layer）**：对输入数据进行处理，可以有多个隐藏层
- **输出层（Output Layer）**：产生最终的输出结果

### 前馈神经网络（Feedforward Neural Network，FNN）

前馈神经网络是神经网络家族中的基本单元。特点是数据从输入层开始，经过一个或多个隐藏层，最后到达输出层，全过程没有循环或反馈。

**前馈神经网络的基本结构**：
- **输入层**：数据进入网络的入口点
- **隐藏层**：一个或多个层，用于捕获数据的非线性特征
- **输出层**：输出网络的预测结果
- **连接权重与偏置**：每个神经元的输入通过权重进行加权求和，并加上偏置值，然后通过激活函数传递

### 神经网络类型

1. **前馈神经网络（Feedforward Neural Networks）**：数据单向流动，从输入层到输出层，无反馈连接
2. **卷积神经网络（Convolutional Neural Networks, CNNs）**：适用于图像处理，使用卷积层提取空间特征
3. **循环神经网络（Recurrent Neural Networks, RNNs）**：适用于序列数据，如时间序列分析和自然语言处理，允许信息反馈循环
4. **长短期记忆网络（Long Short-Term Memory, LSTM）**：一种特殊的RNN，能够学习长期依赖关系

## nn.Module 基础

### 创建自定义模块

```python
import torch
import torch.nn as nn

class SimpleNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(10, 5)
        self.fc2 = nn.Linear(5, 1)
    
    def forward(self, x):
        x = self.fc1(x)
        x = torch.relu(x)
        x = self.fc2(x)
        return x

# 使用
model = SimpleNet()
x = torch.randn(32, 10)  # batch_size=32, features=10
output = model(x)
print(output.shape)  # (32, 1)
```

### 模块的基本方法

```python
model = SimpleNet()

# 查看模型结构
print(model)

# 获取所有参数
for name, param in model.named_parameters():
    print(f"{name}: {param.shape}")

# 获取所有模块
for name, module in model.named_modules():
    print(f"{name}: {module}")

# 获取所有子模块
for name, child in model.named_children():
    print(f"{name}: {child}")

# 移动到 GPU
model = model.cuda()
# 或
model = model.to('cuda')

# 设置为训练/评估模式
model.train()   # 训练模式（启用 dropout 等）
model.eval()    # 评估模式（禁用 dropout 等）
```

## 常用层类型

### 全连接层（Linear）

```python
# Linear(in_features, out_features, bias=True)
fc = nn.Linear(784, 128)
x = torch.randn(32, 784)
out = fc(x)
print(out.shape)  # (32, 128)
```

### 卷积层（Conv2d）

```python
# Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0)
conv = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)
x = torch.randn(32, 3, 32, 32)  # (batch, channels, height, width)
out = conv(x)
print(out.shape)  # (32, 64, 32, 32)

# 其他卷积层
conv1d = nn.Conv1d(16, 32, kernel_size=3)
conv3d = nn.Conv3d(1, 32, kernel_size=3)
```

### 池化层

```python
# 最大池化
maxpool = nn.MaxPool2d(kernel_size=2, stride=2)
x = torch.randn(32, 64, 32, 32)
out = maxpool(x)
print(out.shape)  # (32, 64, 16, 16)

# 平均池化
avgpool = nn.AvgPool2d(kernel_size=2, stride=2)

# 自适应池化（输出固定大小）
adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))
```

### 归一化层

```python
# Batch Normalization
bn = nn.BatchNorm2d(64)
x = torch.randn(32, 64, 32, 32)
out = bn(x)

# Layer Normalization
ln = nn.LayerNorm(128)
x = torch.randn(32, 128)
out = ln(x)

# Group Normalization
gn = nn.GroupNorm(8, 64)  # 8 groups, 64 channels
```

### 激活函数

```python
# ReLU
relu = nn.ReLU()
x = torch.randn(32, 128)
out = relu(x)

# Sigmoid
sigmoid = nn.Sigmoid()
out = sigmoid(x)

# Tanh
tanh = nn.Tanh()
out = tanh(x)

# Leaky ReLU
leaky_relu = nn.LeakyReLU(negative_slope=0.01)

# GELU
gelu = nn.GELU()

# 也可以直接使用函数形式
out = torch.relu(x)
out = torch.sigmoid(x)
```

### Dropout 层

```python
# Dropout（训练时随机置零，评估时不变）
dropout = nn.Dropout(p=0.5)
x = torch.randn(32, 128)
out = dropout(x)

# Dropout2d（用于卷积层）
dropout2d = nn.Dropout2d(p=0.5)
```

### 循环神经网络层

```python
# LSTM
lstm = nn.LSTM(input_size=10, hidden_size=20, num_layers=2)
x = torch.randn(5, 32, 10)  # (seq_len, batch, input_size)
out, (h_n, c_n) = lstm(x)
print(out.shape)  # (5, 32, 20)

# GRU
gru = nn.GRU(input_size=10, hidden_size=20, num_layers=2)
out, h_n = gru(x)

# RNN
rnn = nn.RNN(input_size=10, hidden_size=20, num_layers=2)
out, h_n = rnn(x)
```

### 嵌入层（Embedding）

```python
# Embedding(num_embeddings, embedding_dim)
embedding = nn.Embedding(1000, 128)  # 1000个词，每个128维
x = torch.randint(0, 1000, (32, 10))  # 32个样本，每个10个词
out = embedding(x)
print(out.shape)  # (32, 10, 128)
```

### 转置卷积（Transpose Convolution）

```python
# ConvTranspose2d（用于上采样）
transpose_conv = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)
x = torch.randn(32, 64, 16, 16)
out = transpose_conv(x)
print(out.shape)  # (32, 32, 32, 32)
```

## 构建完整模型

### 示例1：全连接网络

```python
class MLP(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super().__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, output_size)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.2)
    
    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.fc2(x)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.fc3(x)
        return x

model = MLP(784, 256, 10)
```

### 示例2：卷积神经网络

```python
class CNN(nn.Module):
    def __init__(self, num_classes=10):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        
        self.pool = nn.MaxPool2d(2, 2)
        self.relu = nn.ReLU()
        
        self.fc1 = nn.Linear(128 * 4 * 4, 512)
        self.fc2 = nn.Linear(512, num_classes)
        self.dropout = nn.Dropout(0.5)
    
    def forward(self, x):
        # Conv layers
        x = self.pool(self.relu(self.conv1(x)))  # 32x32 -> 16x16
        x = self.pool(self.relu(self.conv2(x)))  # 16x16 -> 8x8
        x = self.pool(self.relu(self.conv3(x)))  # 8x8 -> 4x4
        
        # Flatten
        x = x.view(x.size(0), -1)
        
        # FC layers
        x = self.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        return x

model = CNN(num_classes=10)
```

### 示例3：使用 Sequential

```python
# 方式1：直接使用 Sequential
model = nn.Sequential(
    nn.Conv2d(3, 32, 3, padding=1),
    nn.ReLU(),
    nn.MaxPool2d(2),
    nn.Conv2d(32, 64, 3, padding=1),
    nn.ReLU(),
    nn.MaxPool2d(2),
    nn.Flatten(),
    nn.Linear(64 * 8 * 8, 128),
    nn.ReLU(),
    nn.Linear(128, 10)
)

# 方式2：在 Module 中使用 Sequential
class Net(nn.Module):
    def __init__(self):
        super().__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 32, 3),
            nn.ReLU(),
            nn.MaxPool2d(2),
        )
        self.classifier = nn.Sequential(
            nn.Linear(32 * 16 * 16, 128),
            nn.ReLU(),
            nn.Linear(128, 10)
        )
    
    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x
```

## 参数初始化

```python
def init_weights(m):
    if isinstance(m, nn.Linear):
        nn.init.xavier_uniform_(m.weight)
        nn.init.zeros_(m.bias)
    elif isinstance(m, nn.Conv2d):
        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')

model = MLP(784, 256, 10)
model.apply(init_weights)

# 或手动初始化
nn.init.xavier_uniform_(model.fc1.weight)
nn.init.zeros_(model.fc1.bias)
```

## 模型组合

### 使用预训练模型

```python
import torchvision.models as models

# 加载预训练模型
resnet = models.resnet18(pretrained=True)

# 修改最后一层
resnet.fc = nn.Linear(resnet.fc.in_features, 10)

# 冻结部分层
for param in resnet.parameters():
    param.requires_grad = False

# 只训练最后一层
for param in resnet.fc.parameters():
    param.requires_grad = True
```

### 模型组合

```python
class CombinedModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.backbone = models.resnet18(pretrained=True)
        self.backbone.fc = nn.Identity()  # 移除分类层
        self.classifier = nn.Linear(512, 10)
    
    def forward(self, x):
        features = self.backbone(x)
        output = self.classifier(features)
        return output
```

## 钩子函数（Hooks）

```python
# 前向钩子
def forward_hook(module, input, output):
    print(f"{module.__class__.__name__} forward")
    print(f"Input shape: {input[0].shape}")
    print(f"Output shape: {output.shape}")

# 注册钩子
hook = model.conv1.register_forward_hook(forward_hook)

# 使用后移除
hook.remove()

# 反向钩子
def backward_hook(module, grad_input, grad_output):
    print(f"{module.__class__.__name__} backward")
    print(f"Grad output: {grad_output[0].shape}")

hook = model.conv1.register_backward_hook(backward_hook)
```

## 模型保存和加载

```python
# 保存整个模型
torch.save(model, 'model.pth')
model = torch.load('model.pth')

# 保存状态字典（推荐）
torch.save(model.state_dict(), 'model_state.pth')
model.load_state_dict(torch.load('model_state.pth'))
```

## 常见模式

### 残差连接

```python
class ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)
        self.relu = nn.ReLU()
        
        # 如果输入输出通道数不同，需要投影
        self.shortcut = nn.Conv2d(in_channels, out_channels, 1) if in_channels != out_channels else nn.Identity()
    
    def forward(self, x):
        residual = self.shortcut(x)
        out = self.relu(self.conv1(x))
        out = self.conv2(out)
        out += residual
        out = self.relu(out)
        return out
```

### 注意力机制

```python
class Attention(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.query = nn.Linear(dim, dim)
        self.key = nn.Linear(dim, dim)
        self.value = nn.Linear(dim, dim)
        self.scale = dim ** -0.5
    
    def forward(self, x):
        q = self.query(x)
        k = self.key(x)
        v = self.value(x)
        
        attn = torch.matmul(q, k.transpose(-2, -1)) * self.scale
        attn = torch.softmax(attn, dim=-1)
        out = torch.matmul(attn, v)
        return out
```

## 第一个神经网络示例

让我们创建一个简单的神经网络来完成二分类任务：

```python
import torch
import torch.nn as nn
import torch.optim as optim

# 定义网络参数
n_in, n_h, n_out, batch_size = 10, 5, 1, 10

# 创建虚拟输入数据和目标数据
x = torch.randn(batch_size, n_in)  # 随机生成输入数据
y = torch.tensor([[1.0], [0.0], [0.0], [1.0], [1.0], 
                  [1.0], [0.0], [0.0], [1.0], [1.0]])  # 目标输出数据

# 创建顺序模型
model = nn.Sequential(
    nn.Linear(n_in, n_h),      # 输入层到隐藏层的线性变换
    nn.ReLU(),                 # 隐藏层的ReLU激活函数
    nn.Linear(n_h, n_out),     # 隐藏层到输出层的线性变换
    nn.Sigmoid()               # 输出层的Sigmoid激活函数
)

# 定义损失函数和优化器
criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)

# 训练循环
for epoch in range(50):
    y_pred = model(x)          # 前向传播，计算预测值
    loss = criterion(y_pred, y)  # 计算损失
    
    optimizer.zero_grad()      # 清零梯度
    loss.backward()            # 反向传播，计算梯度
    optimizer.step()           # 更新模型参数
    
    if (epoch + 1) % 10 == 0:
        print(f'Epoch [{epoch+1}/50], Loss: {loss.item():.4f}')
```

### 使用类定义的方式

```python
class SimpleNN(nn.Module):
    def __init__(self, n_in=10, n_h=5, n_out=1):
        super(SimpleNN, self).__init__()
        # 定义神经网络的层
        self.fc1 = nn.Linear(n_in, n_h)    # 输入层有 n_in 个特征，隐藏层有 n_h 个神经元
        self.fc2 = nn.Linear(n_h, n_out)   # 隐藏层输出到 n_out 个神经元（用于二分类）
        self.sigmoid = nn.Sigmoid()        # 二分类激活函数
    
    def forward(self, x):
        x = torch.relu(self.fc1(x))        # 使用 ReLU 激活函数
        x = self.sigmoid(self.fc2(x))       # 输出层使用 Sigmoid 激活函数
        return x

# 实例化模型
model = SimpleNN()

# 定义损失函数和优化器
criterion = nn.BCELoss()  # 二元交叉熵损失
optimizer = optim.SGD(model.parameters(), lr=0.1)

# 训练
epochs = 100
for epoch in range(epochs):
    # 前向传播
    outputs = model(x)
    loss = criterion(outputs, y)
    
    # 反向传播
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    if (epoch + 1) % 10 == 0:
        print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')
```

## 激活函数（Activation Function）

激活函数决定了神经元是否应该被激活。它们是非线性函数，使得神经网络能够学习和执行更复杂的任务。

常见的激活函数包括：

- **Sigmoid**：用于二分类问题，输出值在 0 和 1 之间
- **Tanh**：输出值在 -1 和 1 之间，常用于输出层之前
- **ReLU（Rectified Linear Unit）**：目前最流行的激活函数之一，定义为 $f(x) = \max(0, x)$，有助于解决梯度消失问题
- **Softmax**：常用于多分类问题的输出层，将输出转换为概率分布

```python
import torch.nn.functional as F

# ReLU 激活
output = F.relu(input_tensor)

# Sigmoid 激活
output = torch.sigmoid(input_tensor)

# Tanh 激活
output = torch.tanh(input_tensor)

# Softmax 激活
output = F.softmax(input_tensor, dim=1)
```

## 训练过程（Training Process）

训练神经网络涉及以下步骤：

1. **准备数据**：通过 `DataLoader` 加载数据
2. **定义损失函数和优化器**
3. **前向传播**：计算模型的输出
4. **计算损失**：与目标进行比较，得到损失值
5. **反向传播**：通过 `loss.backward()` 计算梯度
6. **更新参数**：通过 `optimizer.step()` 更新模型的参数
7. **重复上述步骤**，直到达到预定的训练轮数

## 测试与评估

训练完成后，需要对模型进行测试和评估。

```python
# 设置模型为评估模式
model.eval()

# 在评估过程中禁用梯度计算
with torch.no_grad():
    output = model(x_test)
    loss = criterion(output, y_test)
    print(f'Test Loss: {loss.item():.4f}')
    
    # 对于分类问题，计算准确率
    _, predicted = torch.max(output.data, 1)
    correct = (predicted == y_test).sum().item()
    accuracy = 100 * correct / len(y_test)
    print(f'Accuracy: {accuracy:.2f}%')
```

## 练习

1. 创建一个简单的全连接网络，用于手写数字识别
2. 实现一个 CNN 用于图像分类
3. 创建一个带残差连接的模块
4. 使用预训练的 ResNet 并微调最后一层
5. 实现一个简单的二分类神经网络，并可视化训练过程

## 下一步

掌握了神经网络模块后，学习：
- [数据加载](4-数据加载.md)：如何加载和预处理数据
- [训练流程](5-训练流程.md)：完整的训练和验证循环
