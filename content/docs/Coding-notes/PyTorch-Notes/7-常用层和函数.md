---
title: "常用层和函数"
weight: 7
bookhidden: true
---

# 常用层和函数

PyTorch 提供了丰富的神经网络层和工具函数。本章将介绍常用的层类型和函数，帮助你更好地构建模型。

## 卷积层

### 一维卷积（Conv1d）

```python
import torch.nn as nn

# Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0)
conv1d = nn.Conv1d(16, 32, kernel_size=3, stride=1, padding=1)
x = torch.randn(32, 16, 100)  # (batch, channels, length)
out = conv1d(x)
print(out.shape)  # (32, 32, 100)
```

### 二维卷积（Conv2d）

```python
# Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0)
conv2d = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)
x = torch.randn(32, 3, 32, 32)  # (batch, channels, height, width)
out = conv2d(x)
print(out.shape)  # (32, 64, 32, 32)

# 不同 kernel_size
conv3x3 = nn.Conv2d(3, 64, kernel_size=3)
conv5x5 = nn.Conv2d(3, 64, kernel_size=5)
conv1x1 = nn.Conv2d(3, 64, kernel_size=1)  # 1x1 卷积，用于降维

# 分组卷积
group_conv = nn.Conv2d(64, 128, kernel_size=3, groups=8)  # 8组
```

### 三维卷积（Conv3d）

```python
conv3d = nn.Conv3d(1, 32, kernel_size=3)
x = torch.randn(32, 1, 16, 16, 16)  # (batch, channels, depth, height, width)
out = conv3d(x)
```

### 转置卷积（ConvTranspose）

```python
# 用于上采样
transpose_conv = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)
x = torch.randn(32, 64, 16, 16)
out = transpose_conv(x)
print(out.shape)  # (32, 32, 32, 32)
```

## 池化层

### 最大池化

```python
# MaxPool2d(kernel_size, stride=None, padding=0)
maxpool = nn.MaxPool2d(kernel_size=2, stride=2)
x = torch.randn(32, 64, 32, 32)
out = maxpool(x)
print(out.shape)  # (32, 64, 16, 16)

# 全局最大池化
global_maxpool = nn.AdaptiveMaxPool2d((1, 1))
out = global_maxpool(x)
print(out.shape)  # (32, 64, 1, 1)
```

### 平均池化

```python
avgpool = nn.AvgPool2d(kernel_size=2, stride=2)
x = torch.randn(32, 64, 32, 32)
out = avgpool(x)

# 全局平均池化
global_avgpool = nn.AdaptiveAvgPool2d((1, 1))
out = global_avgpool(x)
```

## 归一化层

### Batch Normalization

```python
# BatchNorm2d(num_features)
bn = nn.BatchNorm2d(64)
x = torch.randn(32, 64, 32, 32)
out = bn(x)

# BatchNorm1d（用于全连接层）
bn1d = nn.BatchNorm1d(128)
x = torch.randn(32, 128)
out = bn1d(x)
```

### Layer Normalization

```python
# LayerNorm(normalized_shape)
ln = nn.LayerNorm(128)
x = torch.randn(32, 128)
out = ln(x)

# 用于2D输入
ln2d = nn.LayerNorm([64, 32, 32])
x = torch.randn(32, 64, 32, 32)
out = ln2d(x)
```

### Group Normalization

```python
# GroupNorm(num_groups, num_channels)
gn = nn.GroupNorm(8, 64)  # 8组，64通道
x = torch.randn(32, 64, 32, 32)
out = gn(x)
```

### Instance Normalization

```python
# InstanceNorm2d(num_features)
in_norm = nn.InstanceNorm2d(64)
x = torch.randn(32, 64, 32, 32)
out = in_norm(x)
```

## 激活函数

### ReLU 系列

```python
# ReLU
relu = nn.ReLU()
x = torch.randn(32, 128)
out = relu(x)

# Leaky ReLU
leaky_relu = nn.LeakyReLU(negative_slope=0.01)

# PReLU（参数化 ReLU）
prelu = nn.PReLU(num_parameters=1)

# RReLU（随机 ReLU）
rrelu = nn.RReLU(lower=0.125, upper=0.3333333333333333)
```

### 其他激活函数

```python
# Sigmoid
sigmoid = nn.Sigmoid()
out = sigmoid(x)

# Tanh
tanh = nn.Tanh()
out = tanh(x)

# GELU
gelu = nn.GELU()
out = gelu(x)

# Swish（通过 SiLU 实现）
swish = nn.SiLU()
out = swish(x)

# ELU
elu = nn.ELU(alpha=1.0)
out = elu(x)
```

## Dropout 层

```python
# Dropout（用于全连接层）
dropout = nn.Dropout(p=0.5)
x = torch.randn(32, 128)
out = dropout(x)

# Dropout2d（用于卷积层）
dropout2d = nn.Dropout2d(p=0.5)
x = torch.randn(32, 64, 32, 32)
out = dropout2d(x)

# Dropout3d
dropout3d = nn.Dropout3d(p=0.5)
```

## 全连接层

```python
# Linear(in_features, out_features, bias=True)
fc = nn.Linear(784, 128)
x = torch.randn(32, 784)
out = fc(x)
print(out.shape)  # (32, 128)

# 多层全连接
class MLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(784, 256)
        self.fc2 = nn.Linear(256, 128)
        self.fc3 = nn.Linear(128, 10)
    
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        return x
```

## 循环神经网络层

### LSTM

```python
# LSTM(input_size, hidden_size, num_layers=1, batch_first=False)
lstm = nn.LSTM(input_size=10, hidden_size=20, num_layers=2, batch_first=True)
x = torch.randn(32, 5, 10)  # (batch, seq_len, input_size)
out, (h_n, c_n) = lstm(x)
print(out.shape)  # (32, 5, 20)
print(h_n.shape)  # (2, 32, 20) - (num_layers, batch, hidden_size)
```

### GRU

```python
gru = nn.GRU(input_size=10, hidden_size=20, num_layers=2, batch_first=True)
x = torch.randn(32, 5, 10)
out, h_n = gru(x)
```

### RNN

```python
rnn = nn.RNN(input_size=10, hidden_size=20, num_layers=2, batch_first=True)
x = torch.randn(32, 5, 10)
out, h_n = rnn(x)
```

## 嵌入层

```python
# Embedding(num_embeddings, embedding_dim)
embedding = nn.Embedding(1000, 128)  # 1000个词，每个128维
x = torch.randint(0, 1000, (32, 10))  # 32个样本，每个10个词
out = embedding(x)
print(out.shape)  # (32, 10, 128)

# 使用预训练权重
pretrained_weights = torch.randn(1000, 128)
embedding = nn.Embedding.from_pretrained(pretrained_weights)
```

## 注意力机制

### 多头注意力

```python
# MultiheadAttention(embed_dim, num_heads)
mha = nn.MultiheadAttention(embed_dim=512, num_heads=8, batch_first=True)
x = torch.randn(32, 10, 512)  # (batch, seq_len, embed_dim)
out, attn_weights = mha(x, x, x)
print(out.shape)  # (32, 10, 512)
print(attn_weights.shape)  # (32, 10, 10)
```

## 实用工具函数

### Flatten

```python
# 展平层
flatten = nn.Flatten()
x = torch.randn(32, 3, 32, 32)
out = flatten(x)
print(out.shape)  # (32, 3072)

# 指定起始维度
flatten = nn.Flatten(start_dim=1)
x = torch.randn(32, 3, 32, 32)
out = flatten(x)
print(out.shape)  # (32, 3072)
```

### Unflatten

```python
# 反展平
unflatten = nn.Unflatten(1, (3, 32, 32))
x = torch.randn(32, 3072)
out = unflatten(x)
print(out.shape)  # (32, 3, 32, 32)
```

### Padding

```python
# 零填充
pad = nn.ZeroPad2d(padding=1)
x = torch.randn(32, 3, 32, 32)
out = pad(x)
print(out.shape)  # (32, 3, 34, 34)

# 反射填充
reflect_pad = nn.ReflectionPad2d(padding=1)

# 复制填充
replicate_pad = nn.ReplicationPad2d(padding=1)
```

## 组合层

### Sequential

```python
# 顺序组合多个层
model = nn.Sequential(
    nn.Conv2d(3, 64, 3, padding=1),
    nn.BatchNorm2d(64),
    nn.ReLU(),
    nn.MaxPool2d(2),
    nn.Conv2d(64, 128, 3, padding=1),
    nn.BatchNorm2d(128),
    nn.ReLU(),
    nn.MaxPool2d(2),
    nn.Flatten(),
    nn.Linear(128 * 8 * 8, 256),
    nn.ReLU(),
    nn.Linear(256, 10)
)
```

### ModuleList

```python
# 用于动态创建层列表
class Net(nn.Module):
    def __init__(self):
        super().__init__()
        self.layers = nn.ModuleList([
            nn.Linear(10, 20),
            nn.Linear(20, 30),
            nn.Linear(30, 40)
        ])
    
    def forward(self, x):
        for layer in self.layers:
            x = layer(x)
        return x
```

### ModuleDict

```python
# 用于创建层字典
class Net(nn.Module):
    def __init__(self):
        super().__init__()
        self.layers = nn.ModuleDict({
            'conv': nn.Conv2d(3, 64, 3),
            'bn': nn.BatchNorm2d(64),
            'relu': nn.ReLU()
        })
    
    def forward(self, x):
        x = self.layers['conv'](x)
        x = self.layers['bn'](x)
        x = self.layers['relu'](x)
        return x
```

## 常用函数

### torch.nn.functional

```python
import torch.nn.functional as F

# 激活函数
x = F.relu(x)
x = F.sigmoid(x)
x = F.tanh(x)
x = F.gelu(x)

# 卷积
x = F.conv2d(x, weight, bias, stride=1, padding=1)

# 池化
x = F.max_pool2d(x, kernel_size=2, stride=2)
x = F.avg_pool2d(x, kernel_size=2, stride=2)
x = F.adaptive_avg_pool2d(x, (1, 1))

# Dropout
x = F.dropout(x, p=0.5, training=self.training)

# 归一化
x = F.batch_norm(x, running_mean, running_var, weight, bias, training=True)

# 插值（上采样）
x = F.interpolate(x, size=(64, 64), mode='bilinear', align_corners=False)
```

### 损失函数（在下一章详细介绍）

```python
# 交叉熵
loss = F.cross_entropy(outputs, targets)

# 均方误差
loss = F.mse_loss(outputs, targets)

# 二元交叉熵
loss = F.binary_cross_entropy(outputs, targets)
```

## 构建复杂模型示例

### ResNet Block

```python
class ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride=stride, padding=1)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU(inplace=True)
        
        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, 1, stride=stride),
                nn.BatchNorm2d(out_channels)
            )
    
    def forward(self, x):
        residual = self.shortcut(x)
        out = self.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += residual
        out = self.relu(out)
        return out
```

### Transformer Block

```python
class TransformerBlock(nn.Module):
    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):
        super().__init__()
        self.attention = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)
        self.norm1 = nn.LayerNorm(embed_dim)
        self.norm2 = nn.LayerNorm(embed_dim)
        
        self.feed_forward = nn.Sequential(
            nn.Linear(embed_dim, ff_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(ff_dim, embed_dim),
            nn.Dropout(dropout)
        )
    
    def forward(self, x):
        # 自注意力
        attn_out, _ = self.attention(x, x, x)
        x = self.norm1(x + attn_out)
        
        # 前馈网络
        ff_out = self.feed_forward(x)
        x = self.norm2(x + ff_out)
        
        return x
```

## 练习

1. 使用不同的层构建一个 CNN 模型
2. 实现一个带残差连接的模块
3. 构建一个简单的 Transformer 块
4. 尝试使用不同的归一化层，比较效果

## 下一步

掌握了常用层和函数后，学习：
- [优化器与损失函数](8-优化器与损失函数.md)：了解如何优化模型
- [实战案例](9-实战案例.md)：完整的项目示例
